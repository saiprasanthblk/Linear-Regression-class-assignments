View(M)
seq(from=-4, to=8, by=0.1)
length(seq(from=-4, to=8, by=0.1))
for (y in 1:121) {
for (i in seq(from=-4, to=8, by=0.1)) {
for (p in 1:13) {
M[y, p] = K((i-xi[p])/bandwidth)
p = p + 1
}
i = i + 1
}
}
View(M)
View(M)
M[, 1]
sum(M[, 1])
sum(M[1, 0])
sum(M[1, ])
for (y in 1:121) {
for (i in seq(from=-4, to=8, by=0.1)) { # I am incrementing by 0.1 to capture many values
for (p in 1:13) {
M[y, p] = K((i-xi[p])/bandwidth)
p = p + 1
}
i = i + 1
}
plotting = cbind(i,sum(M[y, ]))
}
View(plotting)
for (y in 1:121) {
for (i in seq(from=-4, to=8, by=0.1)) { # I am incrementing by 0.1 to capture many values
for (p in 1:13) {
M[y, p] = K((i-xi[p])/bandwidth)
p = p + 1
}
i = i + 1
}
plotting = cbind(i,sum(M[y, ]))
y = y + 1
}
View(plotting)
for (y in 1:121) {
for (i in seq(from=-4, to=8, by=0.1)) { # I am incrementing by 0.1 to capture many values
for (p in 1:13) {
M[y, p] = K((i-xi[p])/bandwidth)
p = p + 1
}
i = i + 1
}
}
for (y in 1:121) {
for (i in seq(from=-4, to=8, by=0.1)) { # I am incrementing by 0.1 to capture many values
for (p in 1:13) {
M[y, p] = K((i-xi[p])/bandwidth)
}
}
}
K_sum = NA
for (y in 1:121) {
for (i in seq(from=-4, to=8, by=0.1)) { # I am incrementing by 0.1 to capture many values
for (p in 1:13) {
M[y, p] = K((i-xi[p])/bandwidth)
K_sum = sum(M[y, ])
}
}
}
K_sum = data.frame(matrix(NA))
for (y in 1:121) {
for (i in seq(from=-4, to=8, by=0.1)) { # I am incrementing by 0.1 to capture many values
for (p in 1:13) {
M[y, p] = K((i-xi[p])/bandwidth)
K_sum = sum(M[y, ])
}
}
}
for (y in 1:121) {
for (d in seq(from=-4, to=8, by=0.1)) {
plotting = cbind(d, sum(M[y, ]))
}
}
View(plotting)
plotting = data.frame(matrix(NA))
for (y in 1:121) {
for (d in seq(from=-4, to=8, by=0.1)) {
plotting = cbind(d, sum(M[y, ]))
}
}
View(plotting)
d = seq(from=-4, to=8, by=0.1)
K_sum = apply(M,2,sum)
K_sum
K_sum = apply(M,1,sum)
K_sum
library (ISLR)
set.seed (1)
train=sample (392 ,196)
install.packages(ISLR)
library (ISLR)
set.seed (1)
train=sample (392 ,196)
install.packages(ISLR)
install.packages('ISLR')
install.packages('ISLR')
library (ISLR)
set.seed (1)
train=sample (392 ,196)
set.seed (2)
train=sample (392 ,196)
lm.fit =lm(mpg∼horsepower ,subset =train)
lm.fit =lm(mpg∼horsepower ,data=Auto ,subset =train )
lm.fit =lm(mpg∼horsepower ,subset =train)
attach (Auto)
mean((mpg -predict (lm.fit ,Auto))[-train ]^2)
lm.fit2=lm(mpg∼poly(horsepower ,2) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit2 ,Auto))[-train ]^2)
# 19.82
lm.fit3=lm(mpg∼poly(horsepower ,3) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit3 ,Auto))[-train ]^2)
# 19.78
lm.fit =lm(mpg~horsepower ,data=Auto ,subset =train )
attach (Auto)
mean((mpg -predict (lm.fit ,Auto))[-train ]^2)
lm.fit2=lm(mpg∼poly(horsepower ,2) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit2 ,Auto))[-train ]^2)
# 19.82
lm.fit3=lm(mpg∼poly(horsepower ,3) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit3 ,Auto))[-train ]^2)
# 19.78
library (ISLR)
set.seed (1)
train=sample (392 ,196)
lm.fit =lm(mpg~horsepower ,data=Auto ,subset =train )
attach (Auto)
mean((mpg -predict (lm.fit ,Auto))[-train ]^2)
lm.fit2=lm(mpg∼poly(horsepower ,2) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit2 ,Auto))[-train ]^2)
# 19.82
lm.fit3=lm(mpg∼poly(horsepower ,3) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit3 ,Auto))[-train ]^2)
# 19.78
library (boot)
cv.error=rep (0,5)
for (i in 1:5){
+ glm.fit=glm(mpg∼poly(horsepower ,i),data=Auto)
+ cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
+ }
cv.error
cv.error=rep (0,5)
for (i in 1:5) {
+ glm.fit=glm(mpg∼poly(horsepower ,i),data=Auto)
+ cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
+ }
cv.error
cv.error=rep (0,5)
for (i in 1:5) {
+ glm.fit=glm(mpg∼poly(horsepower ,i),data=Auto)
+ cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
+ }
cv.error
cv.error=rep (0,5)
cv.error=rep (0,5)
for (i in 1:5) {
glm.fit=glm(mpg∼poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
}
cv.error
cv.error=rep (0,5)
for (i in 1:5) {
glm.fit=glm(mpg∼poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
}
cv.error
for (i in 1:5) {
glm.fit=glm(mpg∼poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
}
cv.error
cv.error=rep (0,5)
for (i in 1:5) {
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
}
cv.error
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
cv.error=rep (0,5)
for (i in 1:5)
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
cv.error
cv.error=rep (0,5)
for (i in 1:5) {
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
}
cv.error
cv.error=rep (0,5)
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
}
cv.error
cv.error=rep (0,5)
i =1
for (i in 1:5) {
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
}
cv.error
set.seed (17)
cv.error .10= rep (0 ,10)
for (i in 1:10) {
+ glm.fit=glm(mpg∼poly(horsepower ,i),data=Auto)
+ cv.error .10[i]=cv.glm (Auto ,glm .fit ,K=10) $delta [1]
+ }
cv.error .10
cv.error=rep (0,5)
for (i in 1:5) {
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
}
cv.error
cv.error=rep (0,5)
for (i in 1:5) {
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
}
cv.error
cv.error=rep (0,5)
for (i in 1:5) {
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta[1]
}
cv.error
cv.error=rep (0,5)
for (i in 1:5) {
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm (Auto,glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10= rep (0 ,10)
for (i in 1:10) {
glm.fit=glm(mpg∼poly(horsepower,i),data=Auto)
cv.error .10[i]=cv.glm (Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
set.seed(17)
cv.error.10=rep(0 ,10)
for (i in 1:10) {
glm.fit=glm(mpg∼poly(horsepower,i),data=Auto)
cv.error .10[i]=cv.glm (Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10) {
glm.fit=glm(mpg∼poly(horsepower,i),data=Auto)
cv.error .10[i]=cv.glm (Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10) {
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error .10[i]=cv.glm (Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10) {
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm (Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
alpha.fn=function (data ,index){
X=data$X[index]
Y=data$Y[index]
return ((var(Y)-cov(X,Y))/(var(X)+var(Y) -2* cov(X,Y)))
}
alpha.fn(Portfolio ,1:100)
set.seed (1)
alpha.fn(Portfolio ,sample (100 ,100 , replace =T))
boot(Portfolio ,alpha.fn,R=1000)
boot(Auto ,boot.fn ,1000)
# ORDINARY NONPARAMETRIC BOOTSTRAP
#Call:
boot(data = Auto , statistic = boot.fn, R = 1000)
# Bootstrap Statistics :
#  original bias std. error
boot(Auto ,boot.fn ,1000)
# ORDINARY NONPARAMETRIC BOOTSTRAP
#Call:
#  boot(data = Auto , statistic = boot.fn, R = 1000)
# Bootstrap Statistics :
boot(data = Auto , statistic = boot.fn, R = 1000)
boot(Auto ,boot.fn ,1000)
boot(data = Portfolio , statistic = alpha.fn, R = 1000)
boot.fn=function (data ,index )
return (coef(lm(mpg∼horsepower ,data=data ,subset=index)))
boot.fn(Auto ,1:392)
set.seed(1)
boot.fn(Auto ,sample (392 ,392 , replace =T))
boot.fn(Auto ,sample (392 ,392 , replace =T))
boot(Auto ,boot.fn ,1000)
boot.fn=function (data ,index )
coefficients(lm(mpg∼horsepower +I( horsepower ^2) ,data=data ,
subset =index))
set.seed (1)
boot(Auto ,boot.fn ,1000)
x <- 1:100000
plot(x, 1 - (1 - 1/x)^x)
x = c(1:100000)
plot(x, 1 - (1 - 1/x)^x)
library(ISLR)
attach(Default)
set.seed(1)
fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial")
summary(fit.glm)
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
summary(fit.glm)
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
model2.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
summary(model2.glm)
probability <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm <- rep("No", length(probability))
pred.glm[probability > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm <- rep("No", length(probs))
pred.glm[probs > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm <- rep("No", length(probs))
pred.glm[probs > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm <- rep("No", length(probs))
pred.glm[probs > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
model3.glm <- glm(default ~ income + balance + student, data = Default, family = "binomial", subset = train)
pred.glm <- rep("No", length(probability))
probability <- predict(model3.glm, newdata = Default[-train, ], type = "response")
pred.glm[probability > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
model3.glm <- glm(default ~ income + balance + student, data = Default, family = "binomial", subset = train)
pred.glm <- rep("No", length(probability))
probability <- predict(model3.glm, newdata = Default[-train, ], type = "response")
pred.glm[probability > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
summary (lm(mpg∼horsepower +I(horsepower ^2) ,data=Auto))$coef
set.seed(1)
y <- rnorm(100)
x <- rnorm(100)
y <- x - 2 * x^2 + rnorm(100)
plot(x, y)
library(boot)
set.seed(1)
Data <- data.frame(x, y)
fit.glm.1 <- glm(y ~ x)
cv.glm(Data, fit.glm.1)$delta[1]
fit.glm.2 <- glm(y ~ poly(x, 2))
cv.glm(Data, fit.glm.2)$delta[1]
fit.glm.3 <- glm(y ~ poly(x, 3))
cv.glm(Data, fit.glm.3)$delta[1]
fit.glm.4 <- glm(y ~ poly(x, 4))
cv.glm(Data, fit.glm.4)$delta[1]
library(boot)
set.seed(1)
Data <- data.frame(x, y)
model1.glm <- glm(y ~ x)
cv.glm(Data, model1.glm)$delta[1]
model2.glm <- glm(y ~ poly(x, 2))
cv.glm(Data, fit.model2)$delta[1]
model2.glm <- glm(y ~ poly(x, 2))
cv.glm(Data, fit.model2.glm)$delta[1]
model2.glm <- glm(y ~ poly(x, 2))
cv.glm(Data, model2.glm)$delta[1]
model3.glm <- glm(y ~ poly(x, 3))
cv.glm(Data, model3.glm)$delta[1]
model4.glm <- glm(y ~ poly(x, 4))
cv.glm(Data, model4.glm)$delta[1]
set.seed(10)
model1.glm <- glm(y ~ x)
cv.glm(Data, model1.glm)$delta[1]
model2.glm <- glm(y ~ poly(x, 2))
cv.glm(Data, model2.glm)$delta[1]
model3.glm <- glm(y ~ poly(x, 3))
cv.glm(Data, model3.glm)$delta[1]
fit.glm.4 <- glm(y ~ poly(x, 4))
cv.glm(Data, fit.glm.4)$delta[1]
model4.glm <- glm(y ~ poly(x, 4))
cv.glm(Data, model4.glm)$delta[1]
summary(model2.glm)
summary(model2.glm)
summary(model3.glm)
summary(model4.glm)
summary(model2.glm)
library(readxl)
data1 <- read_excel("data-table-B1.xls")
setwd("C:/Users/Prasanth/Downloads/M.S. Data Science Academics/1st Sem/Linear Regression/Assignments/Assignment 3")
data1 <- read_excel("data-table-B1.xls")
model1 <- lm(x2 ~ x7 + x8, data = data1)
summary(model1)
model1 <- lm(y ~ x2 + x7 + x8, data = data1)
summary(model1)
anova(model1)
summary(model1)
confint(lm(y ~ x7, data = data1))
confint(lm(y ~ x2 + x7 + x8, data = data1))
df <- data.frame(x2 = 2300, x7 = 56, x8 = 2100)
predict(model1, df, interval="confidence", level=0.95)
model2 <- lm(y ~ x7 + x8, data = data1)
summary(model2)
df2 <- data1
predict(model2, df2, interval="confidence", level=0.95)
predict(model1, df, interval="confidence", level=0.95)
confint(lm(y ~ x7 + x8, data = data1))
df <- data.frame(x7 = 56, x8 = 2100)
df2 <- data.frame(x7 = 56, x8 = 2100)
predict(model2, df2, interval="confidence", level=0.95)
data3 <- read_excel("data-table-B3")
data3 <- read_excel("data-table-B3.xls")
model3 <- lm(y ~ x1 + x6)
data2 <- read_excel("data-table-B3.xls")
model3 <- lm(y ~ x1 + x6, data = data2)
summary(model3)
confint(lm(y ~ x1 + x6, data = data2))
df3 <- data.frame(x7 = 257, x6 = 2)
predict(model3, df3, interval="confidence", level=0.95)
df3 <- data.frame(x7 = 257, x6 = 2)
predict(model3, df3, interval="confidence", level=0.95)
df3 <- data.frame(x1 = 257, x6 = 2)
predict(model3, df3, interval="confidence", level=0.95)
data4 <- read_excel("data-table-B5.xls")
model4 <- lm(y ~ x6 + x7)
model4 <- lm(y ~ x6 + x7, data = data4)
summary(model4)
confint(model4)
model5 <- lm(y ~ x6, data = data4)
summary(model5)
confint(model5)
summary(model5)
anova(model5)
anova(model4)
data5 <- read_excel("data-table-B16.xls")
View(data5)
model6 <- lm(data5$LifeExp ~ data5$`People-per-TV` + data5$`People-per-Dr`)
summary(model6)
model7 <- lm(data5$LifeExpMale ~ data5$`People-per-TV` + data5$`People-per-Dr`)
summary(model7)
summary(model8)
model8 <- lm(data5$LifeExpFemale ~ data5$`People-per-TV` + data5$`People-per-Dr`)
summary(model8)
summary(model6)
summary(model6)
summary(model7)
summary(model8)
confint(model6)
confint(model7)
confint(model8)
summary(model6)
confint(model8)
summary(model7)
anova(model1)
aov(model1)
sum(model1$residuals^2)
mean(sum(model1$residuals^2))
anova(model1)
aov(model1)
anova(model1)
summary(model1)
summary(model3)
anova(model3)
predict(model3, df3, interval="confidence", level=0.95)
model3 <- lm(y ~ x1 + x6, data = data2)
summary(model3)
anova(model3)
df3 <- data.frame(x1 = 257, x6 = 2)
predict(model3, df3, interval="confidence", level=0.95)
confint(lm(y ~ x1 + x6, data = data2))
predict(model3, df3, interval="confidence", level=0.95)
summary(model3)
df3 <- data.frame(x1 = 257, x6 = 2)
predict(model3, df3, interval="prediction", level=0.95)
predict(model3, df3, interval="confidence", level=0.95)
predict(model3, df3, interval="confidence")
predict(model3, df3, interval="confidence", level = 0.95)
df3 <- data.frame(x1 = 275, x6 = 2)
predict(model3, df3, interval="confidence", level = 0.95)
predict(model3, df3, interval="predict", level = 0.95)
predict(model3, df3, interval="predict", level = 0.95)
model2.4 <- lm(y ~ x1, data = data2)
predict(model3, df3, interval="confidence", level = 0.95)
predict(model2.4, df3, interval="confidence", level = 0.95)
predict(model2.4, df3, interval="predict", level = 0.95)
summary(model4)
anova(model4)
anova(model5)
anova(model5) # Ms. res =
anova(model4) # Ms. res = 2573
summary(model6)
summary(model2)
